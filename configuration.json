{
    "llm": {
        "gpu_util": 0.9,
        "gpu_kv_cache_gb": 3.0,
        "max_model_len": 11000,
        "max_num_seqs": 8,
        "max_num_batched_tokens": 2048,
        "max_output_tokens": 2000,
        "llm_temperature": 0.7,
        "llm_models": [
            "models/Qwen2.5-32B-Instruct-AWQ"
        ]
    },
    "server": {
        "host": "0.0.0.0",
        "port": 8000,
        "reload": false,
        "log_level": "info"
    }
}